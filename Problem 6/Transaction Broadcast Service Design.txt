
After researching and reading up on blockchain and my prior knowledge, what I understand of a transaction broadcast service is that it is a software system that receives requests to broadcast transactions to a blockchain network, for instance BSC, Ethereum or Solana. Transactions must be reliably and securely sent to the broadcast service, so that they can be forwarded to other nodes in the network, ensuring that all other nodes have the same view of the transactions. This is essential so as to prevent double-spending and other types of fraud. It is important that the transaction data do not get lost and we must provide a clear and transparent broadcasting service, that will aim to include important information of a transaction (Recipient, Signature, Value, Gas Limit, Max Fee Per Gas…) in the broadcasted data.

The whole broadcasting system consists of 3 steps. Firstly, the HTTP API service provided is used to request transaction broadcasting. Then, the transaction data received is sent to the broadcaster service to be signed with the user’s private key, proving that the user was the actual owner of assets being transferred. We can use a cryptographic algorithm that uses the private key to encrypt the transaction data. This signed and encrypted data will be outputted back to the API service, to be sent to the transaction broadcasting service.

When the transactions are fed to the broadcasting service, we can employ a Queue data structure to handle the transactions (with a job queueing system with the help of an in-memory data structure store like Redis). The service reads transactions from the job queue and attempts to broadcast them to the network. Successful broadcasted transactions will be dequeued while failed broadcasts will be retried again. An additional feature can also be to rank the transactions based on their importance, that high-value transactions are at the front of the queue to be broadcasted earlier. To prevent overwhelming the network, increasing delays in retrying of broadcasts can be implemented for the failed broadcasted transactions. For instance, the first retry can happen 5 seconds later, while the service continues to broadcast other transactions in the mean time, then its second retry will happen after 1 minute of delay and so on. Perhaps, if the delay time for the retries for a transaction reaches 5 minutes, we can store this failed transaction in a temporary memory and remove it from the broadcasting queue.

A memory can be used to store the broadcast status of our transactions (pass or fail). This is so that the admin (us, the developers) can easily create an admin page to access the failed transactions manually, so that we can send the failed broadcasted transaction back into the broadcasting queue once again, after any errors have been fixed. This easily accessed memory of transactions’ broadcast status allows us to track and retrigger any of them if needed conveniently. A page created can then access this memory (maybe in the form of a Hashtable), to show passed or failed transactions easily, with the feature for the logged in admin to be allowed to broadcast the failed transactions once again. 

Here will be some of the suggested tech programming languages and frameworks that I feel is essential and suitable for this service:

Firstly, I feel that GoLang is a good choice to run this whole backend service as it has good support for concurrency and networking, ensuring that our service runs efficiently and robustly. 

The HTTP API can be supported by frameworks like Express.js or Gin (Googled it but am not really familiar with these yet, so I shall read more about it). Database used to store the transaction information can utilise PostgreSQL. Also, ethers.js can be used to interact with the blockchain nodes, and connect to an evm-compatible blockchain network. Also, as mentioned earlier, the in-memory data structures provided by Redis can be used to manage the transaction broadcasting and retrying. 

Lastly, for deployment, our service can be containerized using Docker, and Docker Swarm can be used to manage scaling and rolling updates to our service.